{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM-RAG: Multimodal Retrieval-Augmented Generation Demo\n",
    "\n",
    "This notebook demonstrates the MM-RAG system for multimodal question answering.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhijoysarkar/mmrag/blob/main/examples/MM_RAG_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MM-RAG\n",
    "!pip install -q git+https://github.com/abhijoysarkar/mmrag.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about famous landmarks\n",
    "documents = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower in Paris, France. Built in 1889, it stands 330 meters tall.\",\n",
    "    \"The Statue of Liberty is located on Liberty Island in New York Harbor. It was a gift from France in 1886.\",\n",
    "    \"The Great Wall of China stretches over 13,000 miles across northern China. Most was built during the Ming Dynasty.\",\n",
    "    \"The Taj Mahal is an ivory-white marble mausoleum in Agra, India. It was built by Shah Jahan in 1632.\",\n",
    "    \"The Colosseum is an ancient amphitheater in Rome, Italy. Built in 80 AD, it could hold 50,000 spectators.\",\n",
    "    \"Machu Picchu is a 15th-century Inca citadel in Peru. It sits at 2,430 meters above sea level.\",\n",
    "    \"The Golden Gate Bridge spans the Golden Gate strait in San Francisco. Opened in 1937, it's painted International Orange.\",\n",
    "    \"The Sydney Opera House is a performing arts center in Sydney, Australia. Its unique sail-like design was completed in 1973.\",\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base: {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents[:3], 1):\n",
    "    print(f\"{i}. {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize MM-RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmrag.models.vision_encoder import VisionEncoder\n",
    "from mmrag.models.retriever import FaissRetriever\n",
    "from mmrag.models.fusion import CrossModalFusionBlock\n",
    "\n",
    "# Initialize components (CPU for Colab free tier)\n",
    "print(\"Loading vision encoder...\")\n",
    "vision_encoder = VisionEncoder(\n",
    "    model_name=\"openai/clip-vit-base-patch16\",\n",
    "    device=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Loading retriever...\")\n",
    "retriever = FaissRetriever(\n",
    "    dim=512,  # CLIP dimension\n",
    "    text_encoder_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    device='auto'\n",
    ")\n",
    "\n",
    "print(\"Loading fusion module...\")\n",
    "fusion = CrossModalFusionBlock(\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    fusion_type='attention'\n",
    ")\n",
    "\n",
    "print(\"‚úì All components loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and index documents\n",
    "print(\"Encoding documents...\")\n",
    "doc_embeddings_384 = retriever.encode_text(documents)\n",
    "\n",
    "# Pad to match CLIP dimension (512)\n",
    "doc_embeddings = np.pad(\n",
    "    doc_embeddings_384,\n",
    "    ((0, 0), (0, 512 - doc_embeddings_384.shape[1])),\n",
    "    mode='constant'\n",
    ")\n",
    "\n",
    "doc_ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "retriever.add(doc_embeddings, doc_ids, documents)\n",
    "\n",
    "print(f\"‚úì Indexed {len(documents)} documents\")\n",
    "print(f\"  Index size: {retriever.index.ntotal} vectors\")\n",
    "print(f\"  Embedding dimension: {retriever.dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tower_image():\n",
    "    \"\"\"Create a simple tower-like structure.\"\"\"\n",
    "    img = Image.new('RGB', (512, 512), color='lightblue')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Draw tower\n",
    "    draw.rectangle([(200, 450), (312, 470)], fill='darkgray', outline='black', width=2)\n",
    "    points = [(180, 450), (256, 300), (332, 450)]\n",
    "    draw.polygon(points, fill='gray', outline='black')\n",
    "    points = [(220, 300), (256, 200), (292, 300)]\n",
    "    draw.polygon(points, fill='darkgray', outline='black')\n",
    "    points = [(240, 200), (256, 100), (272, 200)]\n",
    "    draw.polygon(points, fill='gray', outline='black')\n",
    "    draw.line([(256, 100), (256, 50)], fill='black', width=3)\n",
    "    \n",
    "    return img\n",
    "\n",
    "test_image = create_tower_image()\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Multimodal Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"What famous tower structure is this and where is it located?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Encode image\n",
    "print(\"Encoding image...\")\n",
    "image_emb = vision_encoder.encode(test_image)\n",
    "print(f\"‚úì Image embedding shape: {image_emb.shape}\")\n",
    "\n",
    "# Hybrid search (text + image)\n",
    "print(\"\\nPerforming hybrid search...\")\n",
    "results = retriever.hybrid_search(\n",
    "    query,\n",
    "    image_emb.cpu().numpy(),\n",
    "    alpha=0.5,  # Balance between text and image\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(f\"‚úì Retrieved {len(results)} documents\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP RETRIEVED DOCUMENTS:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc_id, score, text) in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. [{doc_id}] Score: {score:.4f}\")\n",
    "    print(f\"   {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cross-Modal Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embeddings for fusion\n",
    "text_emb = torch.tensor(retriever.encode_text(query), device=vision_encoder.device)\n",
    "\n",
    "# Get document embeddings\n",
    "doc_texts = [r[2] for r in results]\n",
    "doc_embs_raw = retriever.encode_text(doc_texts)\n",
    "doc_embs = np.pad(\n",
    "    doc_embs_raw,\n",
    "    ((0, 0), (0, 512 - doc_embs_raw.shape[1])),\n",
    "    mode='constant'\n",
    ")\n",
    "doc_embs = torch.tensor(doc_embs, device=vision_encoder.device)\n",
    "\n",
    "# Reshape for fusion module\n",
    "if image_emb.dim() == 2:\n",
    "    image_emb = image_emb.unsqueeze(1)  # (1, 1, 512)\n",
    "if text_emb.dim() == 2:\n",
    "    text_emb = text_emb.unsqueeze(1)  # (1, 1, 512)\n",
    "if doc_embs.dim() == 2:\n",
    "    doc_embs = doc_embs.unsqueeze(0)  # (1, K, 512)\n",
    "\n",
    "print(f\"Image embedding: {image_emb.shape}\")\n",
    "print(f\"Text embedding: {text_emb.shape}\")\n",
    "print(f\"Document embeddings: {doc_embs.shape}\")\n",
    "\n",
    "# Fuse modalities\n",
    "print(\"\\nFusing modalities with attention...\")\n",
    "with torch.no_grad():\n",
    "    fused_emb = fusion(image_emb, text_emb, doc_embs)\n",
    "\n",
    "print(f\"‚úì Fused embedding shape: {fused_emb.shape}\")\n",
    "print(f\"‚úì Fused embedding norm: {fused_emb.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MM-RAG PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Query: {query}\")\n",
    "print(f\"\\nüñºÔ∏è  Image: Tower structure (512x512)\")\n",
    "print(f\"\\nüìö Knowledge Base: {len(documents)} documents\")\n",
    "print(f\"\\nüîç Retrieved Documents: {len(results)}\")\n",
    "print(f\"\\nüéØ Top Match: {results[0][2][:100]}...\")\n",
    "print(f\"   Relevance Score: {results[0][1]:.4f}\")\n",
    "print(f\"\\nüîÄ Fusion Type: Attention-based\")\n",
    "print(f\"   Output Dimension: {fused_emb.shape[-1]}\")\n",
    "print(\"\\n‚úÖ Pipeline executed successfully!\")\n",
    "print(\"\\n‚ÑπÔ∏è  Note: This demo shows retrieval + fusion only.\")\n",
    "print(\"   For text generation, connect a language model (LLaMA, GPT, etc.)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Your Own Image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own image or load from URL\n",
    "from google.colab import files\n",
    "\n",
    "# Option 1: Upload\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    custom_image = Image.open(filename)\n",
    "    display(custom_image)\n",
    "    \n",
    "    # Run retrieval\n",
    "    custom_query = input(\"Enter your query: \")\n",
    "    custom_emb = vision_encoder.encode(custom_image)\n",
    "    custom_results = retriever.hybrid_search(custom_query, custom_emb.cpu().numpy(), top_k=3)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    for i, (_, score, text) in enumerate(custom_results, 1):\n",
    "        print(f\"{i}. [{score:.4f}] {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Add Generation**: Connect a language model for full MM-RAG\n",
    "- **Fine-tune**: Train LoRA adapters on your domain\n",
    "- **Deploy**: Use the FastAPI server for production\n",
    "- **Extend**: Add more fusion strategies or encoders\n",
    "\n",
    "### Resources\n",
    "- [GitHub Repository](https://github.com/abhijoysarkar/mmrag)\n",
    "- [Documentation](https://github.com/abhijoysarkar/mmrag#readme)\n",
    "- [Paper: RAG and Multimodal Learning](https://arxiv.org/abs/2005.11401)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
